{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwimwznZCrJJ"
      },
      "source": [
        "**Setup Ollama**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01Cy3JcVCqBZ",
        "outputId": "f5d377ec-35c7-4577-bb5a-ff3438dfabfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.7/96.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !curl -fsSL https://ollama.com/install.sh | sh\n",
        "# !nohup ollama serve &\n",
        "# !ollama pull qwen3:4b\n",
        "!pip install -q mcp-server sentence-transformers  PyMuPDF\n",
        "!pip install -q gradio\n",
        "!pip install -q langchain-community\n",
        "!pip install -U --quiet langgraph\n",
        "!pip install -q python-docx\n",
        "!pip install -q chromadb\n",
        "!pip install -q faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgvqvsEVH4_W",
        "outputId": "122f0e23-7358-45f1-da9d-caa0dac699e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20250327 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rh5sucWBDMhU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\anaconda3\\envs\\mcp_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import re\n",
        "import uuid\n",
        "import json\n",
        "import queue\n",
        "import threading\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Any\n",
        "\n",
        "import gradio as gr\n",
        "import pdfplumber\n",
        "import docx\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8aVK_pfdIqyk"
      },
      "outputs": [],
      "source": [
        "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "# from langchain_community.vectorstores import Chroma\n",
        "# from langchain_core.documents import Document\n",
        "\n",
        "# # Use a BGE embedding model from HuggingFace\n",
        "# bge_embedding = HuggingFaceEmbeddings(\n",
        "#     model_name=\"BAAI/bge-m3\",\n",
        "#     encode_kwargs={\"normalize_embeddings\": True}\n",
        "# )\n",
        "\n",
        "# vectorstore = Chroma.from_documents(doc_splits, bge_embedding, persist_directory=\"./chroma_db\")\n",
        "# retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZPnRAoo-DOCI"
      },
      "outputs": [],
      "source": [
        "# from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "# retriever_tool = create_retriever_tool(\n",
        "#     retriever,\n",
        "#     name=\"search_user_documents\",\n",
        "#     description=\"Searches the user's uploaded documents and returns the most relevant passages with page and paragraph and document info.\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HRS9XPAjMdb3"
      },
      "outputs": [],
      "source": [
        "# retriever_tool.invoke({\"query\": \"what is LIME ?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VN4pd0DfQThy"
      },
      "outputs": [],
      "source": [
        "# from langchain_community.llms.ollama import Ollama\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# # model = Ollama(model=\"Qwen/Qwen1.5-14B-Chat-GPTQ-Int4\")\n",
        "# # MODEL_NAME = \"Qwen/Qwen1.5-14B-Chat-GPTQ-Int4\"\n",
        "\n",
        "# # tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "# # model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "# # pipe = pipeline(\n",
        "# #     \"text-generation\",\n",
        "# #     model=model,\n",
        "# #     tokenizer=tokenizer,\n",
        "# #     max_new_tokens=1024,\n",
        "# #     temperature=0,\n",
        "# #     do_sample=True,\n",
        "# # )\n",
        "\n",
        "# # response_model = Ollama(model=\"Qwen/Qwen1.5-14B-Chat-GPTQ-Int4\")\n",
        "# response_model = Ollama(model=\"qwen3:4b\")\n",
        "\n",
        "# def generate_query_or_respond(state):\n",
        "#     user_message = state[\"messages\"][-1][\"content\"]\n",
        "#     # 1. Run retriever/tool for context\n",
        "#     retrieved_docs = retriever_tool.run(user_message)  # Or retriever.get_relevant_documents(...)\n",
        "#     context = \"\\n\\n\".join([\n",
        "#         doc.page_content if hasattr(doc, \"page_content\") else str(doc)\n",
        "#         for doc in retrieved_docs\n",
        "#     ])\n",
        "#     # 2. Compose prompt for LLM\n",
        "#     prompt = f\"Use only the following context to answer:\\n{context}\\n\\nUser question: {user_message}\\nAnswer:\"\n",
        "#     # 3. Get LLM response\n",
        "#     answer = response_model.invoke(prompt)\n",
        "#     # 4. Return as a new message\n",
        "#     return {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": answer}]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VkQPb14_R3U5"
      },
      "outputs": [],
      "source": [
        "# input = {\n",
        "#     \"messages\": [\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": \"What does the paper talk about?\",\n",
        "#         }\n",
        "#     ]\n",
        "# }\n",
        "# output = generate_query_or_respond(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "49vABvD_xT08"
      },
      "outputs": [],
      "source": [
        "# print(output[\"messages\"][-1][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVDpsO9Gw22I"
      },
      "source": [
        "**Handle file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_vKrEL_Rw2dw"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def get_file_bytes_and_name(pdf_file):\n",
        "    print(\"DEBUG: pdf_file type:\", type(pdf_file))\n",
        "    print(\"DEBUG: pdf_file dir:\", dir(pdf_file))\n",
        "    print(\"DEBUG: pdf_file repr:\", repr(pdf_file))\n",
        "    # Standard file-like object (e.g. Python's open, script mode)\n",
        "    if hasattr(pdf_file, \"read\"):\n",
        "         return pdf_file.read(), Path(pdf_file.name).name\n",
        "    if isinstance(pdf_file, str):\n",
        "        file_path = Path(pdf_file)\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            return f.read(), file_path.name\n",
        "    # else:\n",
        "    #     raise ValueError(\"Could not extract file bytes from uploaded file.\")\n",
        "    raise ValueError(\"Could not extract file bytes from uploaded file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "owNLWf8SqIXj"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import Document\n",
        "import pdfplumber\n",
        "import re\n",
        "\n",
        "def pdf_to_documents(pdf_path: str):\n",
        "    \"\"\"\n",
        "    Return one Document per paragraph with clean spacing and page/paragraph\n",
        "    metadata.  Fixes the 'wordsGluedTogether' problem by converting line breaks\n",
        "    to single spaces *before* splitting paragraphs.\n",
        "    \"\"\"\n",
        "    docs = []\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_no, page in enumerate(pdf.pages, 1):\n",
        "\n",
        "            # 1️  primary attempt – keep layout spacing\n",
        "            raw = page.extract_text(x_tolerance=1.5, y_tolerance=1.5, layout=True) or \"\"\n",
        "\n",
        "            # 2️  fallback – default extraction if primary is empty\n",
        "            if not raw.strip():\n",
        "                raw = page.extract_text() or \"\"\n",
        "\n",
        "            # 3️  ***KEY FIX: replace line breaks with single spaces***\n",
        "            raw = raw.replace(\"\\n\", \" \")\n",
        "            raw = re.sub(r\"\\s{2,}\", \" \", raw)      # collapse double spaces\n",
        "\n",
        "            # 4️  build Document objects (simple paragraph split)\n",
        "            for para_no, para in enumerate(raw.split(\". \"), 1):\n",
        "                para = para.strip()\n",
        "                if para:\n",
        "                    docs.append(\n",
        "                        Document(\n",
        "                            page_content=para,\n",
        "                            metadata={\"page\": page_no, \"paragraph\": para_no},\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "    # 5️  sanity-check: warn if *every* paragraph still lacks spaces\n",
        "    if docs and not any(\" \" in d.page_content for d in docs):\n",
        "        raise ValueError(\n",
        "            \"Text extraction produced paragraphs with no word boundaries. \"\n",
        "            \"The PDF may be scanned or use an unusual layout.\"\n",
        "        )\n",
        "\n",
        "    return docs\n",
        "\n",
        "# def pdf_to_documents(pdf_path: str) -> list[Document]:\n",
        "#     docs = []\n",
        "#     with pdfplumber.open(pdf_path) as pdf:\n",
        "#         for page_no, page in enumerate(pdf.pages, start=1):\n",
        "#             text = page.extract_text() or \"\"\n",
        "#             # simple paragraph split – tweak as you like\n",
        "#             for para_no, para in enumerate(text.split(\"\\n\\n\"), start=1):\n",
        "#                 cleaned = para.strip()\n",
        "#                 if cleaned:           # skip blank chunks\n",
        "#                     docs.append(\n",
        "#                         Document(\n",
        "#                             page_content=cleaned,\n",
        "#                             metadata={\"page\": page_no, \"paragraph\": para_no}\n",
        "#                         )\n",
        "#                     )\n",
        "#     return docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UEWd7KuhvVxT"
      },
      "outputs": [],
      "source": [
        "def format_sources(retrieved_docs):\n",
        "    numbered, exposed = [], []\n",
        "    for i, doc in enumerate(retrieved_docs, start=1):\n",
        "        p, n = doc.metadata.get(\"page\"), doc.metadata.get(\"paragraph\")\n",
        "        txt   = doc.page_content\n",
        "        numbered.append(f\"({i}) [page {p} ¶{n}] {txt}\")\n",
        "        exposed.append(f\"[page {p} ¶{n}] {txt}\")\n",
        "    return \"\\n\".join(numbered), exposed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mY4vVZ_TE6w"
      },
      "source": [
        "**Create Vector database**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w3tJxMUJ-xq8"
      },
      "outputs": [],
      "source": [
        "def build_vectorstore_from_pdf(pdf_file, embedding_model, persist_directory=None):\n",
        "    import tempfile\n",
        "    from pathlib import Path\n",
        "    import pdfplumber\n",
        "    from langchain.docstore.document import Document\n",
        "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "    from langchain_community.vectorstores import Chroma\n",
        "    import re\n",
        "\n",
        "    # --- read bytes & filename ---\n",
        "    file_bytes, file_name = get_file_bytes_and_name(pdf_file)\n",
        "\n",
        "    # --- write to a temp file ---\n",
        "    temp_path = Path(tempfile.gettempdir()) / file_name\n",
        "    with open(temp_path, \"wb\") as f:\n",
        "        f.write(file_bytes)\n",
        "\n",
        "        # --- NEW: create one-paragraph Documents with location metadata ---\n",
        "    docs_list = pdf_to_documents(str(temp_path))      # ← replaces the old pdfplumber loop\n",
        "\n",
        "    # (optional) keep your existing splitter so long paragraphs still get trimmed\n",
        "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=700, chunk_overlap=100\n",
        "    )\n",
        "    doc_splits = splitter.split_documents(docs_list)  # metadata is copied to every chunk\n",
        "\n",
        "    # --- build the Chroma vector store exactly as before ---\n",
        "    vectordb = Chroma.from_documents(\n",
        "        doc_splits,\n",
        "        embedding_model,\n",
        "        persist_directory=persist_directory,\n",
        "    )\n",
        "    return vectordb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IFvnxMrppmK"
      },
      "source": [
        "**Create tool**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fIvKm_F-povl"
      },
      "outputs": [],
      "source": [
        "def build_retriever_tool(vectorstore, name=\"search_user_documents\"):\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    from langchain.tools.retriever import create_retriever_tool\n",
        "    retriever_tool = create_retriever_tool(\n",
        "        retriever,\n",
        "        name=name,\n",
        "        description=\"Searches uploaded documents and returns relevant passages.\"\n",
        "    )\n",
        "    return retriever_tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "1slqDWwQs_RI"
      },
      "outputs": [],
      "source": [
        "# retrieved, _ = retriever.similarity_search_with_score(query, k=3)\n",
        "# numbered_block, exposed_sources = format_sources([r[0] for r in retrieved])\n",
        "# retrieved = retriever_tool.invoke(query)\n",
        "# numbered_block, exposed_sources = format_sources(retrieved)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "_Uf5TAD-p3oL"
      },
      "outputs": [],
      "source": [
        "# !ollama pull qwen3:4b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "bmnqwAGNf2ST"
      },
      "outputs": [],
      "source": [
        "# !pip install -U langchain-deepseek   # tiny wrapper for DeepSeek’s OpenAI-style API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DQa1yFXuwTnM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"NEBIUS_API_KEY\"] = \"eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDExMTc0OTg1MDIyNTg1OTk4NjQ4MCIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTkwNjYyODk1NywidXVpZCI6IjdlY2M0ZjZmLTM1N2YtNDUxZC05ZjNhLWNjYzNlNDIxZGVkYiIsIm5hbWUiOiJIYWNrYXRob24iLCJleHBpcmVzX2F0IjoiMjAzMC0wNi0wMlQxMTowOToxNyswMDAwIn0.XAqOc-I9MTAOnXgR94ii1ZYV7f4nJIcEUMsXroKUjnE\"   # add to ~/.bashrc or HF Space secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q42dfIHh7Ig",
        "outputId": "a0d68079-b562-49ca-90ce-ad625898d9ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Key seen by Python: eyJhbGciOi ...\n"
          ]
        }
      ],
      "source": [
        "print(\"Key seen by Python:\", os.getenv(\"NEBIUS_API_KEY\")[:10], \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZxQB7YXkX7z",
        "outputId": "7827768b-2779-49e1-9ca4-0cedb2384c17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.82.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf1hUNfbpv3E",
        "outputId": "1f3c0c3a-97fa-4172-de3d-8cfbe713da00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.llms.ollama import Ollama\n",
        "import traceback\n",
        "import torch\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-m3\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True, \"device\": device} # Specify device\n",
        ")\n",
        "# response_model = Ollama(model=\"qwen3:4b\")\n",
        "# response_model = ChatDeepSeek(\n",
        "#     model_name=\"deepseek-ai/DeepSeek-V3-0324-fast\",   # or DeepSeek-R1\n",
        "#     base_url=\"https://api.studio.nebius.ai/v1\",\n",
        "#     openai_api_key=os.getenv(\"NEBIUS_API_KEY\"),\n",
        "#     temperature=0.6\n",
        "# )\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\")\n",
        ")\n",
        "\n",
        "# response_model = client.chat.completions.create(\n",
        "#     model=\"deepseek-ai/DeepSeek-V3-0324-fast\",\n",
        "#     max_tokens=512,\n",
        "#     temperature=0.5,\n",
        "#     top_p=0.95,\n",
        "#     messages=[\n",
        "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant provide a concrete answer based on the file user upload.\"},\n",
        "#         {\"role\": \"user\",   \"content\": \"Hello, Nebius! How are you today?\"}\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "def pdf_qa_pipeline(pdf_file, query_text, k=3):\n",
        "    try:\n",
        "        # 1. Build vectorstore\n",
        "        vectorstore  = build_vectorstore_from_pdf(pdf_file, embedding_model)\n",
        "        retriever    = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
        "        retrieved_docs = retriever.get_relevant_documents(query_text)\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            return \"No relevant information found in the uploaded document.\"\n",
        "\n",
        "        numbered_block, exposed_sources = format_sources(retrieved_docs)\n",
        "        if not numbered_block.strip():\n",
        "            return \"No readable excerpts could be extracted from the PDF.\"\n",
        "\n",
        "        # 2. Construct chat messages (no duplication, no old prompt var)\n",
        "        system_msg = (\n",
        "            \"You are a helpful assistant that must answer **only** from the files \"\n",
        "            \"uploaded by the user.\\n\"\n",
        "            \"• Always write full sentences.\\n\"\n",
        "            \"• Put ONE space after every period, question-mark, or exclamation-point.\\n\"\n",
        "            \"• Start a new paragraph only when the topic changes.\\n\"\n",
        "            'If the answer is not present, reply exactly: \"Not found in the document.\" '\n",
        "            \"and then offer a brief recommendation based on your own knowledge.\"\n",
        "        )\n",
        "        user_msg = (\n",
        "            f\"EXCERPTS:\\n{numbered_block}\\n\\n\"\n",
        "            f\"QUESTION: {query_text}\\n\"\n",
        "            \"ANSWER:\"\n",
        "        )\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"deepseek-ai/DeepSeek-V3-0324-fast\",\n",
        "            temperature=0.5,\n",
        "            max_tokens=512,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_msg},\n",
        "                {\"role\": \"user\",   \"content\": user_msg},\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        answer_text = completion.choices[0].message.content.strip()\n",
        "        answer_text = re.sub(r'(?<!\\w)[“”](?!\\w)', '', answer_text)   # drop lone “ or ”\n",
        "        answer_text = re.sub(r'“\\s*”', '', answer_text)\n",
        "        # answer_text = re.sub(r'“\\s*”', '', answer_text)\n",
        "        return f\"{answer_text}\\n\\n**Paragraphs used**\\n\" + \"\\n\".join(exposed_sources)\n",
        "\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "\n",
        "\n",
        "##old code that work\n",
        "# def pdf_qa_pipeline(pdf_file, query_text, k=3):\n",
        "#     try:\n",
        "#         vectorstore  = build_vectorstore_from_pdf(pdf_file, embedding_model)\n",
        "\n",
        "#         #  Get real Document objects (with metadata) straight from Chroma\n",
        "#         retriever     = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
        "#         retrieved_docs = retriever.get_relevant_documents(query_text)\n",
        "#         # └── each item here is a Document, so .metadata works\n",
        "\n",
        "#         if not retrieved_docs:\n",
        "#             return \"No relevant information found in the uploaded document.\"\n",
        "\n",
        "#         numbered_block, exposed_sources = format_sources(retrieved_docs)\n",
        "\n",
        "#         prompt = f\"\"\"You are a helpful assistant that must answer **only** from the files uploaded by user.\n",
        "# • Always write full sentences.\n",
        "# • Put ONE space after every period, question-mark, or exclamation-point.\n",
        "# • Start a new paragraph only when the topic changes.\n",
        "# If the answer is not present, reply exactly: \"Not found in the document.\" and recommend user based on your knowledge.\n",
        "\n",
        "# EXCERPTS:\n",
        "# {numbered_block}\n",
        "\n",
        "# QUESTION: {query_text}\n",
        "# ANSWER:\"\"\"\n",
        "\n",
        "#         # --- SDK call replaces .invoke() ---\n",
        "#         completion = client.chat.completions.create(\n",
        "#             model=\"deepseek-ai/DeepSeek-V3-0324-fast\",\n",
        "#             temperature=0.5,\n",
        "#             max_tokens=512,\n",
        "#             messages=[\n",
        "#                 {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "#                 {\"role\": \"user\",   \"content\": prompt}\n",
        "#             ]\n",
        "#         )\n",
        "#         answer_text = completion.choices[0].message.content.strip()\n",
        "#         return f\"{answer_text}\\n\\n**Paragraphs used**\\n\" + \"\\n\".join(exposed_sources)\n",
        "\n",
        "\n",
        "# #         answer = response_model.invoke(prompt).strip()\n",
        "# #         return f\"{answer}\\n\\n**Paragraphs used**\\n\" + \"\\n\".join(exposed_sources)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         traceback.print_exc()\n",
        "#         return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "xaPq3jpYiM3f"
      },
      "outputs": [],
      "source": [
        "# print(response_model.to_json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "ld25i1F4p4my",
        "outputId": "16f6949c-62d8-4f91-96f0-01cc36f94049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n",
            "\n",
            "🔨 MCP server (using SSE) running at: http://127.0.0.1:7860/gradio_api/mcp/sse\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: pdf_file type: <class 'gradio.utils.NamedString'>\n",
            "DEBUG: pdf_file dir: ['__add__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'name', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n",
            "DEBUG: pdf_file repr: 'C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Temp\\\\gradio\\\\c20bfafb09a6bcaf4ab5359e29617ec0f654f75810e7164f62a2753c926b0fd4\\\\LoRA.pdf'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_33340\\2546135917.py:44: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrieved_docs = retriever.get_relevant_documents(query_text)\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: pdf_file type: <class 'gradio.utils.NamedString'>\n",
            "DEBUG: pdf_file dir: ['__add__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'name', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n",
            "DEBUG: pdf_file repr: 'C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Temp\\\\gradio\\\\ba592b1d2b51051b10947caa6502d85f6011aa06915790aa310d9dc63eb74016\\\\1706.03762v7.pdf'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n",
            "CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        }
      ],
      "source": [
        "demo = gr.Interface(\n",
        "    fn=pdf_qa_pipeline,\n",
        "    inputs=[\n",
        "        gr.File(label=\"Upload PDF\", file_types=[\".pdf\"]),\n",
        "        gr.Textbox(label=\"Question\", placeholder=\"Ask anything about the PDF...\"),\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Answer\"),\n",
        "    title=\"PDF Q&A - All In One\",\n",
        "    description=\"Upload a PDF and ask a question—get an LLM answer grounded only in your document with page number and paragraph provided.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(\n",
        "        mcp_server=True,\n",
        "        show_error=True,\n",
        "        show_api=True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QZ0TeLbArsd",
        "outputId": "35e37d15-4e56-4363-d248-7c06e1e62694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio_client in /usr/local/lib/python3.11/dist-packages (1.10.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio_client) (2025.3.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio_client) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from gradio_client) (0.32.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio_client) (24.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio_client) (4.13.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio_client) (15.0.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio_client) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio_client) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio_client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio_client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio_client) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio_client) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio_client) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio_client) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio_client) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio_client) (1.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.24.1->gradio_client) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.3->gradio_client) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.3->gradio_client) (2.4.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install --upgrade gradio_client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To8EE-jT6ypl",
        "outputId": "535cd01f-9f4b-483e-de6b-1c5d7e7708f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://9a792c22a32f701d86.gradio.live/ ✔\n",
            "Client.predict() Usage Info\n",
            "---------------------------\n",
            "Named API endpoints: 1\n",
            "\n",
            " - predict(pdf_file, query_text, api_name=\"/predict\") -> answer\n",
            "    Parameters:\n",
            "     - [File] pdf_file: filepath (required)  (The FileData class is a subclass of the GradioModel class that represents a file object within a Gradio interface. It is used to store file data and metadata when a file is uploaded.\n",
            "\n",
            "Attributes:\n",
            "    path: The server file path where the file is stored.\n",
            "    url: The normalized server URL pointing to the file.\n",
            "    size: The size of the file in bytes.\n",
            "    orig_name: The original filename before upload.\n",
            "    mime_type: The MIME type of the file.\n",
            "    is_stream: Indicates whether the file is a stream.\n",
            "    meta: Additional metadata used internally (should not be changed).) \n",
            "     - [Textbox] query_text: str (required)  \n",
            "    Returns:\n",
            "     - [Textbox] answer: str \n",
            "\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: pdf_file type: <class 'gradio.utils.NamedString'>\n",
            "DEBUG: pdf_file dir: ['__add__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'name', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n",
            "DEBUG: pdf_file repr: '/tmp/gradio/1deb96a706a1b3be1b4e80d6a5f1963ee6bbb7aa284d50fca78e1c47eca72409/2405.10523v1.pdf'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, I need to figure out what a large language model is based on the provided excerpts. Let me start by reading through the excerpts carefully.\n",
            "\n",
            "Looking at excerpt (1) and (2), they both mention \"Large Language Models (LLMs)\" in the context of text classification. The abstract says that LLMs have revolutionized the field of NLP. They are used as text classifiers and are compared to traditional methods. The paper introduces the Smart Expert System which leverages LLMs. \n",
            "\n",
            "In excerpt (3), there's more detail. It talks about LLMs like PaLM, LLaMA, GPT, and BERT. These models are pre-trained on massive text data and have a lot of parameters, hundreds of billions to trillions. They are larger than traditional DL methods and have stronger language understanding and generation capabilities. They learn rich language representations through pretraining, allowing them to adapt quickly to various tasks. Examples given include cross-lingual sentiment analysis and other NLP tasks.\n",
            "\n",
            "So, putting this together, a large language model is a type of AI model that's trained on vast amounts of text data, has a massive number of parameters, and is capable of understanding and generating human language. They are used for tasks like text classification, sentiment analysis, and can adapt to new tasks with fewer resources compared to traditional methods. The key points from the excerpts are their size, pre-training on large datasets, and capabilities in language understanding and generation.\n",
            "</think>\n",
            "\n",
            "A large language model (LLM) is a type of artificial intelligence model trained on vast amounts of text data, characterized by a massive number of parameters (hundreds of billions to trillions). These models excel in understanding and generating human language, enabling tasks like text classification, sentiment analysis, and cross-lingual processing. They are pre-trained on extensive corpora, allowing them to adapt quickly to diverse downstream tasks with fewer resource-intensive steps compared to traditional methods. Examples include models like GPT, BERT, LLaMA, and PaLM, which leverage extensive pretraining for rich language representations.\n",
            "\n",
            "**Paragraphs used**\n",
            "[page 1 ¶1] Smart Expert System: Large Language Models as\n",
            "Text Classifiers\n",
            "Zhiqiang Wang, Yiran Pang, Yanbin Lin\n",
            "Department of Electrical Engineering and Computer Science, Florida Atlantic University\n",
            "Boca Raton, FL 33431, USA\n",
            "{zwang2022, ypang2022, liny2020}@fau.edu\n",
            "Abstract—Text classification is a fundamental task in Natural using LLMs for intelligent diagnoses. DePalma et al. (2024)\n",
            "Language Processing (NLP), and the advent of Large Language examined LLMs’ code refactoring capabilities. Additionally,\n",
            "Models (LLMs) has revolutionized the field. This paper intro-\n",
            "Boubker (2024) and Oh et al. (2024) evaluated and enhanced\n",
            "ducestheSmartExpertSystem,anovelapproachthatleverages\n",
            "student learning performance using LLMs.\n",
            "LLMs as text classifiers. The system simplifies the traditional\n",
            "text classification workflow, eliminating the need for extensive Although LLMs have been widely applied and researched\n",
            "preprocessinganddomainexpertise.Theperformanceofseveral forvarioustasks,theirpotentialforhandlingtextclassification\n",
            "LLMs, machine learning (ML) algorithms, and neural network tasks remains to be further explored. Recent studies indicate\n",
            "(NN) based structures is evaluated on four datasets. Results\n",
            "that while LLMs perform excellently in syntactic analysis,\n",
            "demonstrate that certain LLMs surpass traditional methods\n",
            "there are still numerous challenges in achieving efficient\n",
            "in sentiment analysis, spam SMS detection and multi-label\n",
            "classification. Furthermore, it is shown that the system’s perfor- and accurate text classification in practical applications. The\n",
            "mance can be further enhanced through few-shot or fine-tuning studies by Betianu et al. (2024) and Møller et al. (2024)\n",
            ",\n",
            "strategies,makingthefine-tunedmodelthetopperformeracross demonstrate the application of LLMs in domain adaptation\n",
            "alldatasets.SourcecodeanddatasetsareavailableinthisGitHub\n",
            "and synthetic data augmentation, revealing both the strengths\n",
            "repository: https://github.com/yeyimilk/llm-zero-shot-classifiers.\n",
            "and limitations of these models in handling rare categories\n",
            "Index Terms—Large Language Models, Text Classification,\n",
            "Natural Language Processing, Smart Expert Systems, Few-Shot or domain shifts. However, the development of LLMs offers\n",
            "Learning, Fine-Tuning, Chat GPT-4, Llama3. new research opportunities to simplify and optimize the text\n",
            "classification process.\n",
            "I. INTRODUCTION Traditional ML and NN approaches to text classification\n",
            "Text classification is a key task in NLP, widely used in involve a multi-stage pipeline that can be both complex\n",
            "areas like sentiment analysis Liu (2022), topic labeling Chen and resource-intensive. These stages typically include feature\n",
            "et al. (2020), question answering Minaee et al. (2021), and extraction, dimensionality reduction, classifier selection, and\n",
            "dialog act classification Qin et al. (2020). Traditional ML model evaluation Kowsari et al. (2019). Figure 1 illustrates\n",
            "[page 1 ¶1] Smart Expert System: Large Language Models as\n",
            "Text Classifiers\n",
            "Zhiqiang Wang, Yiran Pang, Yanbin Lin\n",
            "Department of Electrical Engineering and Computer Science, Florida Atlantic University\n",
            "Boca Raton, FL 33431, USA\n",
            "{zwang2022, ypang2022, liny2020}@fau.edu\n",
            "Abstract—Text classification is a fundamental task in Natural using LLMs for intelligent diagnoses. DePalma et al. (2024)\n",
            "Language Processing (NLP), and the advent of Large Language examined LLMs’ code refactoring capabilities. Additionally,\n",
            "Models (LLMs) has revolutionized the field. This paper intro-\n",
            "Boubker (2024) and Oh et al. (2024) evaluated and enhanced\n",
            "ducestheSmartExpertSystem,anovelapproachthatleverages\n",
            "student learning performance using LLMs.\n",
            "LLMs as text classifiers. The system simplifies the traditional\n",
            "text classification workflow, eliminating the need for extensive Although LLMs have been widely applied and researched\n",
            "preprocessinganddomainexpertise.Theperformanceofseveral forvarioustasks,theirpotentialforhandlingtextclassification\n",
            "LLMs, machine learning (ML) algorithms, and neural network tasks remains to be further explored. Recent studies indicate\n",
            "(NN) based structures is evaluated on four datasets. Results\n",
            "that while LLMs perform excellently in syntactic analysis,\n",
            "demonstrate that certain LLMs surpass traditional methods\n",
            "there are still numerous challenges in achieving efficient\n",
            "in sentiment analysis, spam SMS detection and multi-label\n",
            "classification. Furthermore, it is shown that the system’s perfor- and accurate text classification in practical applications. The\n",
            "mance can be further enhanced through few-shot or fine-tuning studies by Betianu et al. (2024) and Møller et al. (2024)\n",
            ",\n",
            "strategies,makingthefine-tunedmodelthetopperformeracross demonstrate the application of LLMs in domain adaptation\n",
            "alldatasets.SourcecodeanddatasetsareavailableinthisGitHub\n",
            "and synthetic data augmentation, revealing both the strengths\n",
            "repository: https://github.com/yeyimilk/llm-zero-shot-classifiers.\n",
            "and limitations of these models in handling rare categories\n",
            "Index Terms—Large Language Models, Text Classification,\n",
            "Natural Language Processing, Smart Expert Systems, Few-Shot or domain shifts. However, the development of LLMs offers\n",
            "Learning, Fine-Tuning, Chat GPT-4, Llama3. new research opportunities to simplify and optimize the text\n",
            "classification process.\n",
            "I. INTRODUCTION Traditional ML and NN approaches to text classification\n",
            "Text classification is a key task in NLP, widely used in involve a multi-stage pipeline that can be both complex\n",
            "areas like sentiment analysis Liu (2022), topic labeling Chen and resource-intensive. These stages typically include feature\n",
            "et al. (2020), question answering Minaee et al. (2021), and extraction, dimensionality reduction, classifier selection, and\n",
            "dialog act classification Qin et al. (2020). Traditional ML model evaluation Kowsari et al. (2019). Figure 1 illustrates\n",
            "[page 1 ¶1] areas like sentiment analysis Liu (2022), topic labeling Chen and resource-intensive. These stages typically include feature\n",
            "et al. (2020), question answering Minaee et al. (2021), and extraction, dimensionality reduction, classifier selection, and\n",
            "dialog act classification Qin et al. (2020). Traditional ML model evaluation Kowsari et al. (2019). Figure 1 illustrates\n",
            "methods,suchaslogisticregression,NaiveBayes,andsupport this conventional workflow, which starts with data collection\n",
            "vector machines, are common but need a lot of labeled data followed by laborious preprocessing steps such as tokeniza-\n",
            "and can only handle known classes Sarker (2021); Wang tion, punctuation removal, stop word filtering, stemming,\n",
            "et al. (2019). Deep learning (DL) methods, including deep andlemmatization.Subsequently,featureextractiontechniques\n",
            "neural networks, recurrent neural networks, and convolutional like bag-of-words, word embeddings, or TF-IDF are applied\n",
            "neural networks, have surpassed traditional ML algorithms before training a classifier to produce the final classification\n",
            "by capturing complex data relationships LeCun et al. (2015); model.\n",
            "Sutskever et al. (2011). However, DL methods also require However, this traditional approach presents several chal-\n",
            "large labeled datasets and face challenges in adapting to new lenges. Firstly, it requires significant domain expertise to ef-\n",
            "tasks without significant retraining. fectivelypreprocessthedataandengineerfeaturesthatcapture\n",
            "LLMs, especially those based on the Transformer architec- the relevant information for the classification task. Secondly,\n",
            "ture like PaLM Chowdhery et al. (2023), LLaMA Touvron each step in the process must be carefully tuned to optimize\n",
            "et al. (2023), GPT Radford et al. (2018), and BERT Devlin performance—a time-consuming endeavor that often involves\n",
            "et al. (2018), have revolutionized NLP. With hundreds of trial-and-errorexperimentation.Lastly,thesemethodsmaynot\n",
            "billionstotrillionsofparameters,thesemodelsarepre-trained generalize well across different datasets or languages without\n",
            "on massive amounts of text data. Compared to traditional substantial reconfiguration.\n",
            "DL methods, LLMs are much larger and exhibit stronger In contrast, LLMs offer a much easier way to do text\n",
            "languageunderstandingandgenerationcapabilities.Theylearn classification, as depicted in Figure 2, that it just includes\n",
            "rich language representations through extensive pretraining, three main blocks: data collection, feeding data directly into\n",
            "enabling them to adapt quickly to various downstream tasks. LLMs, and obtaining classification results from LLMs. This\n",
            "For instance, Pˇriba´nˇ et al. (2024) used LLMs for cross- simplified flow eliminates the need for explicit feature ex-\n",
            "lingual sentiment analysis. Caruccio et al. (2024) explored traction or dimensionality reduction since LLMs inherently\n",
            "1\n",
            "4202\n",
            "yaM\n",
            "71\n",
            "]LC.sc[\n",
            "1v32501.5042:viXra\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client, handle_file\n",
        "\n",
        "# 1 – point to the server: local URL, public URL, or HF Space name\n",
        "client = Client(\"https://9a792c22a32f701d86.gradio.live\")\n",
        "\n",
        "\n",
        "# (optional) discover the endpoints:\n",
        "print(client.view_api())\n",
        "\n",
        "# client.predict(\n",
        "#     pdf_qa_pipeline(\"/content/2405.10523v1.pdf\", \"What is large language model?\" )\n",
        "# )\n",
        "result = client.predict(\n",
        "    handle_file(\"/content/2405.10523v1.pdf\"),\n",
        "    \"What is a large language model?\",\n",
        "    api_name=\"/predict\"\n",
        ")\n",
        "\n",
        "print(result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "eaIw8x-u6zZ3",
        "outputId": "080e73f5-f69d-4bd5-90ad-e064fdc9dbd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://920cec9a4f130937ce.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://920cec9a4f130937ce.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from gradio_client import Client, handle_file\n",
        "\n",
        "def call_my_api(file, question):\n",
        "\n",
        "    client = Client(\"https://31ee9bf9744a1f34d0.gradio.live\")\n",
        "    result = client.predict(\n",
        "        handle_file(file.name),  # file.name is the uploaded temp file path\n",
        "        question,\n",
        "        api_name=\"/predict\"\n",
        "    )\n",
        "    return result\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=call_my_api,\n",
        "    inputs=[\n",
        "        gr.File(label=\"Upload PDF\"),\n",
        "        gr.Textbox(label=\"Ask a question\"),\n",
        "    ],\n",
        "    outputs=\"text\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So6Vzcf8bzo_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mcp_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
