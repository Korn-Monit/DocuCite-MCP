{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Setup Ollama**"
      ],
      "metadata": {
        "id": "NwimwznZCrJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!nohup ollama serve &\n",
        "!ollama pull qwen3:4b\n",
        "!pip install -q mcp-server sentence-transformers  PyMuPDF faiss-cpu\n",
        "!pip install -q gradio\n",
        "!pip install -q langchain-community\n",
        "!pip install -U --quiet langgraph\n",
        "!pip install -q python-docx\n",
        "!pip install -q chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01Cy3JcVCqBZ",
        "outputId": "12aeb713-659c-4014-cb87-2f7e7f228a17"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "nohup: appending output to 'nohup.out'\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgvqvsEVH4_W",
        "outputId": "84ddbcc4-1310-4620-98ef-22dcd8564874"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.6)\n",
            "Requirement already satisfied: pdfminer.six==20250327 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250327)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import re\n",
        "import uuid\n",
        "import json\n",
        "import queue\n",
        "import threading\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Any\n",
        "\n",
        "import gradio as gr\n",
        "import pdfplumber\n",
        "import docx\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import requests"
      ],
      "metadata": {
        "id": "rh5sucWBDMhU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "# from langchain_community.vectorstores import Chroma\n",
        "# from langchain_core.documents import Document\n",
        "\n",
        "# # Use a BGE embedding model from HuggingFace\n",
        "# bge_embedding = HuggingFaceEmbeddings(\n",
        "#     model_name=\"BAAI/bge-m3\",\n",
        "#     encode_kwargs={\"normalize_embeddings\": True}\n",
        "# )\n",
        "\n",
        "# vectorstore = Chroma.from_documents(doc_splits, bge_embedding, persist_directory=\"./chroma_db\")\n",
        "# retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "8aVK_pfdIqyk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "# retriever_tool = create_retriever_tool(\n",
        "#     retriever,\n",
        "#     name=\"search_user_documents\",\n",
        "#     description=\"Searches the user's uploaded documents and returns the most relevant passages with page and paragraph and document info.\",\n",
        "# )"
      ],
      "metadata": {
        "id": "ZPnRAoo-DOCI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retriever_tool.invoke({\"query\": \"what is LIME ?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "HRS9XPAjMdb3",
        "outputId": "46a55dc4-8b7d-4283-d62a-8a434c388b39"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'needtoevaluatethemodelasawholebeforedeployingit‚Äúin\\nprediction into a trustworthy one.\\nthe wild‚Äù. To make this decision, users need to be confident\\nIn this work, we propose LIME, a novel explanation tech-\\nthatthemodelwillperformwellonreal-worlddata,according\\nnique that explains the predictions of any classifier in an in-\\n\\nIn this work, we propose LIME, a novel explanation tech-\\nthatthemodelwillperformwellonreal-worlddata,according\\nnique that explains the predictions of any classifier in an in-\\nto the metrics of interest. Currently, models are evaluated\\nterpretableandfaithfulmanner,bylearninganinterpretable\\nusing accuracy metrics on an available validation dataset.\\nmodel locally around the prediction. We also propose a\\n\\nbeing explained. LIME samples instances, gets pre- selecting an appropriate family of explanations from a set of\\ndictions using f, and weighs them by the proximity multiple interpretable model classes, thus adapting to the\\nto the instance being explained (represented here given dataset and the classifier. We leave such exploration\\nby size). The dashed line is the learned explanation for future work, as linear explanations work quite well for\\n\\nnations (LIME). The overall goal of LIME is to identify an elements of x(cid:48)), we recover the sample in the original repre-\\ninterpretable model over the interpretable representation sentationz‚ààRd andobtainf(z),whichisusedasalabel for\\nthat is locally faithful to the classifier. the explanation model. Given this dataset Z of perturbed'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.llms.ollama import Ollama\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# # model = Ollama(model=\"Qwen/Qwen1.5-14B-Chat-GPTQ-Int4\")\n",
        "# # MODEL_NAME = \"Qwen/Qwen1.5-14B-Chat-GPTQ-Int4\"\n",
        "\n",
        "# # tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "# # model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "# # pipe = pipeline(\n",
        "# #     \"text-generation\",\n",
        "# #     model=model,\n",
        "# #     tokenizer=tokenizer,\n",
        "# #     max_new_tokens=1024,\n",
        "# #     temperature=0,\n",
        "# #     do_sample=True,\n",
        "# # )\n",
        "\n",
        "# # response_model = Ollama(model=\"Qwen/Qwen1.5-14B-Chat-GPTQ-Int4\")\n",
        "# response_model = Ollama(model=\"qwen3:4b\")\n",
        "\n",
        "# def generate_query_or_respond(state):\n",
        "#     user_message = state[\"messages\"][-1][\"content\"]\n",
        "#     # 1. Run retriever/tool for context\n",
        "#     retrieved_docs = retriever_tool.run(user_message)  # Or retriever.get_relevant_documents(...)\n",
        "#     context = \"\\n\\n\".join([\n",
        "#         doc.page_content if hasattr(doc, \"page_content\") else str(doc)\n",
        "#         for doc in retrieved_docs\n",
        "#     ])\n",
        "#     # 2. Compose prompt for LLM\n",
        "#     prompt = f\"Use only the following context to answer:\\n{context}\\n\\nUser question: {user_message}\\nAnswer:\"\n",
        "#     # 3. Get LLM response\n",
        "#     answer = response_model.invoke(prompt)\n",
        "#     # 4. Return as a new message\n",
        "#     return {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": answer}]}\n"
      ],
      "metadata": {
        "id": "VN4pd0DfQThy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input = {\n",
        "#     \"messages\": [\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": \"What does the paper talk about?\",\n",
        "#         }\n",
        "#     ]\n",
        "# }\n",
        "# output = generate_query_or_respond(input)\n"
      ],
      "metadata": {
        "id": "VkQPb14_R3U5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(output[\"messages\"][-1][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49vABvD_xT08",
        "outputId": "d7f4fe51-41c4-425d-ba9c-086ab593c997"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, let's try to figure out what the paper is about based on the given text. The user provided a lot of text that seems to be a mix of different parts, possibly from a paper or article. Let me start by breaking down the information.\n",
            "\n",
            "First, there's an ISBN number: ISBN978-1-4503-4232-2/16/08...$15.00. That's probably the book's ISBN, but the text might be a chapter or section from that book. Then there's a DOI: http://dx.doi.org/10.1145/2939672.2939778. That's a Digital Object Identifier, which links to an academic paper. The title mentioned is \"Explaining Predictions is as Important as Accuracy for Machine Learning\" by authors like M. T. Dzindolete, S. A. Petersen, R. A. Pomranky, L. G. Pierce, and H. P. Beck. \n",
            "\n",
            "The paper seems to discuss the importance of explainability in machine learning models. The key points mentioned are that explaining predictions is as crucial as achieving high accuracy. The authors argue that understanding why a model makes certain predictions is vital for trust and usability. They mention that some recent work aims to anticipate failures in machine learning tasks by using specific interpretability criteria. There's also a mention of the role of trust in automation, which ties into why explainability is important. The paper might also discuss how models can be adapted to user handling capabilities and that predictions can have different K values for different instances, which might relate to some parameters or thresholds in the models.\n",
            "\n",
            "Looking at the text, there's a part that says \"The word 'Posting'... have no connection to paper we use a constant value for K, leaving the exploration either Christianity or Atheism.\" That seems like a possible error or a misplaced sentence. Maybe \"Postings\" is a term used in the paper, but it's unclear how it connects. However, the main focus is on the importance of explainability in ML, the role of trust, and the need for models to be interpretable.\n",
            "\n",
            "The conclusion and future work section mentions that the authors discuss the role of trust in automation and that explainability is crucial for effective machine learning. They also mention that some recent work aims to anticipate failures in specific interpretability criteria. The paper might also touch on how models can be adjusted to user handling capabilities and that predictions are made for arbitrary reasons, which could relate to the model's parameters or the data it's trained on.\n",
            "\n",
            "Putting this all together, the paper is about the importance of explainability in machine learning models. It argues that just like accuracy, the ability to explain predictions is essential for trust and effective use of AI systems. The authors discuss the role of trust in automation, mention that some recent research is focused on improving interpretability, and perhaps discuss how models can be adapted to different user needs. The paper also touches on the varying values of K for different instances, which might be a parameter in their model or a way to adjust the model's behavior based on input data.\n",
            "</think>\n",
            "\n",
            "The paper discusses the importance of **explainability in machine learning** and argues that it is as critical as achieving high accuracy. Key points include:  \n",
            "\n",
            "1. **Trust in Automation**: The authors emphasize that trust in automated systems relies on the ability to understand and explain model predictions.  \n",
            "2. **Interpretability Criteria**: Recent work aims to improve interpretability by anticipating failures in machine learning tasks and using specific criteria for interpretability.  \n",
            "3. **Role of K Values**: The paper mentions that different instances may have varying values of a parameter $ K $, highlighting the need for adaptable models.  \n",
            "4. **User-Centric Adaptation**: Models should be tailored to user capabilities, ensuring predictions are meaningful and interpretable.  \n",
            "5. **Critique of Arbitrary Reasoning**: Predictions are often made for arbitrary reasons, underscoring the need for transparent and explainable models.  \n",
            "\n",
            "The paper concludes that explainability is essential for effective machine learning, bridging the gap between technical accuracy and human understanding. It also references future work in this area, such as improving interpretability and addressing challenges in automated systems.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gradio as gr\n",
        "# import tempfile\n",
        "# from pathlib import Path\n",
        "# import pdfplumber\n",
        "\n",
        "# from langchain.docstore.document import Document\n",
        "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "# from langchain_community.vectorstores import Chroma\n",
        "# from langchain.tools.retriever import create_retriever_tool\n",
        "# from langchain_community.llms.ollama import Ollama\n",
        "\n",
        "# embedding_model = HuggingFaceEmbeddings(\n",
        "#     model_name=\"BAAI/bge-m3\",\n",
        "#     encode_kwargs={\"normalize_embeddings\": True}\n",
        "# )\n",
        "# response_model = Ollama(model=\"qwen3:4b\")\n",
        "\n",
        "# def pdf_qa_pipeline(pdf_file, query_text):\n",
        "#     # 1. Save file\n",
        "#     temp_path = Path(tempfile.gettempdir()) / pdf_file.name\n",
        "#     temp_path.write_bytes(pdf_file.read())\n",
        "#     # 2. Extract text\n",
        "#     text = \"\"\n",
        "#     with pdfplumber.open(str(temp_path)) as pdf:\n",
        "#         for page in pdf.pages:\n",
        "#             page_text = page.extract_text()\n",
        "#             if page_text:\n",
        "#                 text += page_text\n",
        "#     # 3. Wrap in Document\n",
        "#     doc_obj = Document(page_content=text, metadata={\"source\": pdf_file.name})\n",
        "#     docs_list = [doc_obj]\n",
        "#     # 4. Split into chunks\n",
        "#     splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "#         chunk_size=300, chunk_overlap=50\n",
        "#     )\n",
        "#     doc_splits = splitter.split_documents(docs_list)\n",
        "#     # 5. Embed and build vectorstore\n",
        "#     vectordb = Chroma.from_documents(doc_splits, embedding_model)\n",
        "#     retriever = vectordb.as_retriever()\n",
        "#     retriever_tool = create_retriever_tool(\n",
        "#         retriever,\n",
        "#         name=\"search_pdf\",\n",
        "#         description=\"Search the uploaded PDF for relevant information.\"\n",
        "#     )\n",
        "#     # 6. Retrieve relevant context\n",
        "#     retrieved_docs = retriever_tool.run(query_text)\n",
        "#     context = \"\\n\\n\".join([\n",
        "#         doc.page_content if hasattr(doc, \"page_content\") else str(doc)\n",
        "#         for doc in retrieved_docs\n",
        "#     ])\n",
        "#     # 7. Prompt LLM and return answer\n",
        "#     prompt = f\"Use only the following context to answer:\\n{context}\\n\\nUser question: {query_text}\\nAnswer:\"\n",
        "#     answer = response_model.invoke(prompt)\n",
        "#     return answer\n",
        "\n",
        "# demo = gr.Interface(\n",
        "#     fn=pdf_qa_pipeline,\n",
        "#     inputs=[\n",
        "#         gr.File(label=\"Upload PDF\", file_types=[\".pdf\"]),\n",
        "#         gr.Textbox(label=\"Question\", placeholder=\"Ask anything about the PDF...\"),\n",
        "#     ],\n",
        "#     outputs=gr.Textbox(label=\"Answer\"),\n",
        "#     title=\"PDF Q&A - All In One\",\n",
        "#     description=\"Upload a PDF and ask a question‚Äîget an LLM answer grounded only in your document.\"\n",
        "# )\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     demo.launch(mcp_server=True)\n"
      ],
      "metadata": {
        "id": "1bTeC1QDoXBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handle file**"
      ],
      "metadata": {
        "id": "mVDpsO9Gw22I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def get_file_bytes_and_name(pdf_file):\n",
        "    print(\"DEBUG: pdf_file type:\", type(pdf_file))\n",
        "    print(\"DEBUG: pdf_file dir:\", dir(pdf_file))\n",
        "    print(\"DEBUG: pdf_file repr:\", repr(pdf_file))\n",
        "    # Standard file-like object (e.g. Python's open, script mode)\n",
        "    if hasattr(pdf_file, \"read\"):\n",
        "         return pdf_file.read(), Path(pdf_file.name).name\n",
        "    if isinstance(pdf_file, str):\n",
        "        file_path = Path(pdf_file)\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            return f.read(), file_path.name\n",
        "    # else:\n",
        "    #     raise ValueError(\"Could not extract file bytes from uploaded file.\")\n",
        "    raise ValueError(\"Could not extract file bytes from uploaded file.\")\n"
      ],
      "metadata": {
        "id": "_vKrEL_Rw2dw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Vector database**"
      ],
      "metadata": {
        "id": "-mY4vVZ_TE6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vectorstore_from_pdf(pdf_file, embedding_model, persist_directory=None):\n",
        "    import tempfile\n",
        "    from pathlib import Path\n",
        "    import pdfplumber\n",
        "    from langchain.docstore.document import Document\n",
        "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "    from langchain_community.vectorstores import Chroma\n",
        "    import re\n",
        "\n",
        "    # --- read bytes & filename ---\n",
        "    file_bytes, file_name = get_file_bytes_and_name(pdf_file)\n",
        "\n",
        "    # --- write to a temp file ---\n",
        "    temp_path = Path(tempfile.gettempdir()) / file_name\n",
        "    with open(temp_path, \"wb\") as f:\n",
        "        f.write(file_bytes)\n",
        "\n",
        "    # Extract text and page number and paragraph\n",
        "    ##new code\n",
        "    # docs_list = []\n",
        "    ##end new code\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(str(temp_path)) as pdf:\n",
        "      ##new code\n",
        "      # for page_num, page in enumerate(pdf.pages):\n",
        "      #   page_text = page.extract_text()\n",
        "      #   if page_text and page_text.strip():\n",
        "      #       doc_obj = Document(\n",
        "      #           page_content=page_text,\n",
        "      #           metadata={\"source\": file_name, \"page\": page_num + 1}\n",
        "      #       )\n",
        "      #       docs_list.append(doc_obj)\n",
        "        ##old code\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text\n",
        "    doc_obj = Document(page_content=text, metadata={\"source\": pdf_file.name})\n",
        "    docs_list = [doc_obj]\n",
        "    # Split\n",
        "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=700, chunk_overlap=100\n",
        "    )\n",
        "    doc_splits = splitter.split_documents(docs_list)\n",
        "    # Build vectorstore\n",
        "    vectordb = Chroma.from_documents(\n",
        "        doc_splits,\n",
        "        embedding_model,\n",
        "        persist_directory=persist_directory,\n",
        "    )\n",
        "    return vectordb\n"
      ],
      "metadata": {
        "id": "w3tJxMUJ-xq8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create tool**"
      ],
      "metadata": {
        "id": "1IFvnxMrppmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_retriever_tool(vectorstore, name=\"search_user_documents\"):\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    from langchain.tools.retriever import create_retriever_tool\n",
        "    retriever_tool = create_retriever_tool(\n",
        "        retriever,\n",
        "        name=name,\n",
        "        description=\"Searches uploaded documents and returns relevant passages.\"\n",
        "    )\n",
        "    return retriever_tool\n"
      ],
      "metadata": {
        "id": "fIvKm_F-povl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull qwen3:4b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Uf5TAD-p3oL",
        "outputId": "fcb7d288-05ce-45e6-e1b4-ccc7050177cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQa1yFXuwTnM",
        "outputId": "f0eb9421-4aab-4f91-dc81-b83b6f1c5b48"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.llms.ollama import Ollama\n",
        "import traceback\n",
        "import torch\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-m3\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True, \"device\": device} # Specify device\n",
        ")\n",
        "response_model = Ollama(model=\"qwen3:4b\")\n",
        "\n",
        "# def pdf_qa_pipeline(pdf_file, query_text):\n",
        "#     vectorstore = build_vectorstore_from_pdf(pdf_file, embedding_model)\n",
        "#     retriever_tool = build_retriever_tool(vectorstore)\n",
        "#     retrieved_docs = retriever_tool.run(query_text)\n",
        "#     context = \"\\n\\n\".join([\n",
        "#         doc.page_content if hasattr(doc, \"page_content\") else str(doc)\n",
        "#         for doc in retrieved_docs\n",
        "#     ])\n",
        "#     prompt = f\"Use only the following context to answer:\\n{context}\\n\\nUser question: {query_text}\\nAnswer:\"\n",
        "#     answer = response_model.invoke(prompt)\n",
        "#     return answer\n",
        "def pdf_qa_pipeline(pdf_file, query_text):\n",
        "    # print(\"TYPE:\", type(pdf_file))\n",
        "    # print(\"DIR:\", dir(pdf_file))\n",
        "    # print(\"REPR:\", repr(pdf_file))\n",
        "    try:\n",
        "        print(\"Starting pipeline...\")\n",
        "        vectorstore = build_vectorstore_from_pdf(pdf_file, embedding_model)\n",
        "        print(\"Vectorstore built.\")\n",
        "        retriever_tool = build_retriever_tool(vectorstore)\n",
        "        print(\"Retriever tool built.\")\n",
        "        retrieved_docs = retriever_tool.run(query_text)\n",
        "        print(\"Retrieved docs:\", retrieved_docs)\n",
        "        context = \"\\n\\n\".join([\n",
        "            doc.page_content if hasattr(doc, \"page_content\") else str(doc)\n",
        "            for doc in retrieved_docs\n",
        "        ])\n",
        "        ##new code\n",
        "        # context_chunks = []\n",
        "        # for doc in retrieved_docs:\n",
        "        #     if hasattr(doc, \"metadata\") and hasattr(doc, \"page_content\"):\n",
        "        #         page = doc.metadata.get('page', '?')\n",
        "        #         para = doc.metadata.get('paragraph', '?')\n",
        "        #         content = doc.page_content\n",
        "        #     else:\n",
        "        #         page = '?'\n",
        "        #         para = '?'\n",
        "        #         content = str(doc)\n",
        "        #     context_chunks.append(f\"[Page {page}, Paragraph {para}] {content}\")\n",
        "        # context = \"\\n\\n\".join(context_chunks)\n",
        "        # ---- 1. Print the context chunk you are sending to the LLM ----\n",
        "        print(\"CONTEXT PASSED TO LLM:\", repr(context[:500]))\n",
        "\n",
        "        # ---- 2. Return a default answer if the context is empty ----\n",
        "        if not context.strip():\n",
        "            return \"No relevant information found in the uploaded document.\"\n",
        "\n",
        "        # ---- 3. Use a stricter prompt ----\n",
        "        prompt = f\"\"\"Answer the user's question **using only** the information provided by the file uploaded by ther user.\n",
        "    Provide both the page and paragraph that you have found to support your response.\n",
        "    If the context does not contain the answer, say 'Not found in the document.'\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query_text}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "        answer = response_model.invoke(prompt)\n",
        "        return answer\n",
        "        # print(\"Context:\", context[:300])\n",
        "        # prompt = f\"Use only the following context to answer:\\n{context}\\n\\nUser question: {query_text}\\nAnswer:\"\n",
        "        # print(\"Prompt:\", prompt[:300])\n",
        "        # answer = response_model.invoke(prompt)\n",
        "        # print(\"LLM Answer:\", answer)\n",
        "        # return answer\n",
        "    except Exception as e:\n",
        "        print(\"ERROR OCCURRED IN pdf_qa_pipeline!\")\n",
        "        traceback.print_exc()\n",
        "        return f\"An error occurred: {str(e)}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf1hUNfbpv3E",
        "outputId": "2bc483af-403d-4c4e-c162-d300916e2c84"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"/content/1602.04938v3.pdf\", \"rb\") as pdf_file:\n",
        "#   bytes_data = pdf_file.read()          # works\n",
        "#   file_name  = pdf_file.name            # works  (e.g., '/content/1602.04938v3.pdf')\n"
      ],
      "metadata": {
        "id": "GOXxSHKLyS-2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"/content/1602.04938v3.pdf\", \"rb\") as pdf_file:\n",
        "#     pdf_qa_pipeline(pdf_file, \"what is LIME ?\")\n",
        "# pdf_qa_pipeline(pdf_file, \"what is LIME ?\")"
      ],
      "metadata": {
        "id": "zHLj9v6frx9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(\n",
        "    fn=pdf_qa_pipeline,\n",
        "    inputs=[\n",
        "        gr.File(label=\"Upload PDF\", file_types=[\".pdf\"]),\n",
        "        gr.Textbox(label=\"Question\", placeholder=\"Ask anything about the PDF...\"),\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Answer\"),\n",
        "    title=\"PDF Q&A - All In One\",\n",
        "    description=\"Upload a PDF and ask a question‚Äîget an LLM answer grounded only in your document.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(mcp_server=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "ld25i1F4p4my",
        "outputId": "300b629e-2834-41f6-8a49-208a298b51e7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7e0e808ae0d8953fd5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "\n",
            "üî® MCP server (using SSE) running at: https://7e0e808ae0d8953fd5.gradio.live/gradio_api/mcp/sse\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7e0e808ae0d8953fd5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-LrdCtnqrU4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}